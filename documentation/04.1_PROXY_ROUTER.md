#  4.1 Proxy: El Router Universal (`proxy.py`)

> **Objetivo**: Explicar c贸mo funciona el cerebro central (Gateway) y el flujo de vida de una petici贸n.
> **Archivo**: `app/routers/proxy.py`

---

## `universal_proxy` (La Orquestaci贸n)
```python
async def universal_proxy(request: Request, ...):
```
Esta funci贸n es el "Main Loop" de cada petici贸n. Sigue una cascada estricta:

1.  **Autenticaci贸n**: `await verify_api_key(auth)`.
    *   Si falla, 401 inmediato.
    *   Si pasa, obtenemos el `tenant_id`.

2.  **Configuraci贸n**: `await get_function_config`.
    *   Carga l铆mites de presupuesto, modelos permitidos y reglas de PII.

3.  **PII Redaction (Entrada)**: `await advanced_redact_pii`.
    *   Si el usuario env铆a una tarjeta de cr茅dito, la limpiamos ANTES de que llegue a la l贸gica del negocio.

4.  **Cach茅 Sem谩ntico (Tier 1)**: `await get_semantic_cache`.
    *   Preguntamos a Redis si ya sabemos la respuesta. Si s铆 -> `X-Cache: HIT` (Latencia < 20ms).

5.  **Arbitraje (Model Swapping)**: `await get_best_provider`.
    *   El motor RL decide si cambiamos el modelo solicitado (`gpt-4`) por uno m谩s barato (`gpt-3.5`) si la tarea es sencilla.

6.  **Ejecuci贸n (LiteLLM)**: `await acompletion(...)`.
    *   Llamada real a la API externa.
    *   Manejo de reintentos autom谩ticos si OpenAI da error 500.

7.  **PII Redaction (Salida)**:
    *   Volvemos a limpiar la respuesta de la IA (porsiacaso alucin贸 datos privados).

8.  **Logging As铆ncrono**: `background_tasks.add_task(log_transaction)`.
    *   Guardamos el recibo en Supabase/Redis sin bloquear la respuesta al usuario.

---

## Manejo de Streaming
El proxy soporta `stream=True`.
*   Usamos un generador as铆ncrono (`stream_generator`) que intercepta los chunks "on the fly".
*   Calculamos tokens en tiempo real contando chunks.
