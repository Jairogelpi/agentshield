# agentshield_core/app/db.py
import os
from app.utils import fast_json as json
import uuid
from supabase import create_client, Client
import redis.asyncio as redis
import asyncio
import logging
import time

from decimal import Decimal
import logging

logger = logging.getLogger("agentshield.db")

# Configuraci√≥n
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_KEY")
REDIS_URL = os.getenv("REDIS_URL")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
redis_client = redis.from_url(REDIS_URL, decode_responses=True)

# Nombre de la cola de seguridad (WAL)
WAL_QUEUE_KEY = "wal:pending_charges"

async def get_current_spend(tenant_id: str, cost_center: str):
    """Lectura optimista desde Redis con Fallback a DB"""
    key = f"spend:{tenant_id}:{cost_center}"
    spend = await redis_client.get(key)
    if spend: return float(spend)
    
    loop = asyncio.get_running_loop()
    def _fetch():
        return supabase.table("cost_centers").select("current_spend").eq("tenant_id", tenant_id).eq("id", cost_center).execute()
    
    res = await loop.run_in_executor(None, _fetch)
    if res.data:
        val = res.data[0]['current_spend']
        await redis_client.set(key, val)
        return float(val)
    return 0.0

async def increment_spend(tenant_id: str, cost_center: str, amount: Decimal, metadata: dict = None):
    """
    1. Actualiza Redis (Velocidad). Soporta importes negativos (Earnings).
    2. Escribe en WAL (Seguridad).
    3. Lanza persistencia as√≠ncrona (Eficiencia).
    """
    spend_key = f"spend:{tenant_id}:{cost_center}"
    
    try:
        # Convert Decimal to float just for Redis (Redis requires float/string)
        # But we keep precision in the WAL payload
        amount_float = float(amount)
        
        # 1. ACTUALIZACI√ìN INSTANT√ÅNEA (Redis - Hot Path)
        # Sigue siendo la fuente de verdad para la limitaci√≥n de tasa (Rate Limiting)
        new_total = await redis_client.incrbyfloat(spend_key, amount_float)
        
        # 2. STREAM BUFFER (Alta Velocidad)
        # En lugar de escribir en WAL y luego en DB, enviamos al Stream.
        # El Worker se encargar√° de la persistencia as√≠ncrona y batched.
        
        event_payload = {
            "tid": tenant_id,
            "cc": cost_center,
            "amt": str(amount),
            "ts": str(time.time()), # xadd espera strings/bytes
            "meta": json.dumps(metadata or {})
        }
        
        # A√±adimos al stream 'billing:stream'
        # MAXLEN ~ 100000 para evitar que explote si el worker muere
        await redis_client.xadd("billing:stream", event_payload, maxlen=100000, approximate=True)
        
        return new_total

    except Exception as e:
        logger.error(f"‚ùå Redis Failure in increment_spend: {e}")
        # Fallback de emergencia: Intentar escribir directo a DB s√≠ncronamente
        # (Esto ralentiza la request pero salva el dinero si Redis falla totalmente)
        await _persist_to_db_core(tenant_id, cost_center, amount)
        return 0.0

async def persist_spend_with_wal(charge: dict, raw_payload: str):
    """
    Intenta guardar en DB y, si tiene √©xito, borra del WAL.
    """
    try:
        # Intentamos guardar en Supabase
        success = await _persist_to_db_core(charge['tid'], charge['cc'], charge['amt'])
        
        if success:
            # ‚úÖ √âXITO: Borramos del WAL (ACK)
            # LREM borra 1 ocurrencia de ese string exacto
            await redis_client.lrem(WAL_QUEUE_KEY, 1, raw_payload)
        else:
            # ‚ö†Ô∏è FALLO L√ìGICO: Se queda en Redis para reintento futuro
            logger.warning(f"DB Write failed inside WAL logic for {charge['tid']}")
            
    except Exception as e:
        logger.error(f"CRITICAL: Async persistence crashed: {e}. Data remains in WAL for recovery.")

async def _persist_to_db_core(tenant_id: str, cost_center: str, amount: Decimal) -> bool:
    """N√∫cleo de escritura en Supabase (RPC)"""
    try:
        loop = asyncio.get_running_loop()
        def _exec():
            return supabase.rpc("increment_spend", {
                "p_tenant_id": tenant_id,
                "p_cc_id": cost_center,
                "p_amount": float(amount) # RPC parameter
            }).execute()
        
        await loop.run_in_executor(None, _exec)
        return True
    except Exception as e:
        logger.error(f"Supabase RPC Error: {e}")
        return False

async def recover_pending_charges():
    """
    üöë RECOVERY WORKER (Se ejecuta al inicio)
    Revisa si quedaron cobros pendientes de un crash anterior y los procesa.
    """
    try:
        # Ver cuantos hay
        count = await redis_client.llen(WAL_QUEUE_KEY)
        if count == 0:
            logger.info("‚úÖ WAL Limpio. Sin cobros hu√©rfanos.")
            return

        logger.warning(f"üö® WAL DETECTADO: Recuperando {count} cobros hu√©rfanos tras reinicio...")
        
        # Procesamos todo el backlog
        # Nota: En un sistema masivo, esto se har√≠a por lotes.
        pending_items = await redis_client.lrange(WAL_QUEUE_KEY, 0, -1)
        
        recovered = 0
        loop = asyncio.get_running_loop()
        
        for raw in pending_items:
            try:
                data = json.loads(raw)
                # Ejecutamos en executor para no bloquear el loop principal si tarda
                # Usamos la funci√≥n rpc directamente
                def _exec_recovery():
                    return supabase.rpc("increment_spend", {
                        "p_tenant_id": data['tid'],
                        "p_cc_id": data['cc'],
                        "p_amount": data['amt']
                    }).execute()
                
                await loop.run_in_executor(None, _exec_recovery)
                
                # Si funciona, borramos
                await redis_client.lrem(WAL_QUEUE_KEY, 1, raw)
                recovered += 1
            except Exception as e:
                logger.error(f"Failed to recover item {raw}: {e}")
                
        logger.info(f"‚úÖ Recuperaci√≥n completada: {recovered}/{count} procesados.")
        
    except Exception as e:
        logger.critical(f"üî• FATAL: Fall√≥ el worker de recuperaci√≥n: {e}")

from datetime import datetime

async def get_function_config(tenant_id: str, func_id: str) -> dict:
    if not func_id: return None
    
    # 1. Cache Key
    key = f"func_conf:{tenant_id}:{func_id}"
    
    # 2. Redis
    cached = await redis_client.get(key)
    if cached: return json.loads(cached)
    
    # 3. Supabase
    loop = asyncio.get_running_loop()
    def _fetch():
        return supabase.table("function_configs")\
            .select("*")\
            .eq("tenant_id", tenant_id)\
            .eq("function_id", func_id)\
            .maybe_single()\
            .execute()
    
    res = await loop.run_in_executor(None, _fetch)
    
    if res.data:
        config = res.data
        
        # --- L√ìGICA DE CENICIENTA (Lazy Reset) ---
        # Si last_used es de ayer y hay gasto acumulado, reiniciamos a 0.
        try:
            last_used_str = config.get('last_used')
            if last_used_str:
                # Ajuste de zona horaria simple (Z -> +00:00 para Python isoformat)
                last_used_dt = datetime.fromisoformat(last_used_str.replace('Z', '+00:00'))
                today = datetime.utcnow().date()
                
                if last_used_dt.date() < today and config.get('current_spend_daily', 0) > 0:
                    # 1. Reiniciamos en background (Fire & Forget)
                    asyncio.create_task(_reset_daily_spend(tenant_id, func_id))
                    # 2. Falseamos el dato localmente para permitir esta request
                    config['current_spend_daily'] = 0.0
        except Exception as e:
            logger.warning(f"Lazy Reset logic check failed: {e}")

        # Guardar en Redis y Actualizar heartbet
        await redis_client.setex(key, 60, json.dumps(config))
        asyncio.create_task(_touch_function_last_used(tenant_id, func_id))
        return config
    
    return None

# Funci√≥n auxiliar necesaria para el reinicio
async def _reset_daily_spend(tid, fid):
    try:
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, lambda: supabase.table("function_configs").update({"current_spend_daily": 0}).eq("tenant_id", tid).eq("function_id", fid).execute())
    except: pass

async def _touch_function_last_used(tid, fid):
    """Actualiza la fecha de uso para saber qu√© funciones est√°n vivas"""
    try:
        loop = asyncio.get_running_loop()
        def _update():
             return supabase.table("function_configs").update({"last_used": "now()"}).eq("tenant_id", tid).eq("function_id", fid).execute()
        await loop.run_in_executor(None, _update)
    except:
        pass